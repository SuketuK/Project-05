{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#from lib.project_5 import load_data_from_database, make_data_dict, general_model, general_transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2 - Identify Salient Features Using $\\ell1$-penalty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE: EACH OF THESE SHOULD BE WRITTEN SOLELY WITH REGARD TO STEP 2 - Identify Features**\n",
    "\n",
    "### Domain and Data\n",
    "\n",
    "**We ran naive Logistic Regression on Madelone data set. We also reviewed coefficients for each feature from 500 features. Some of the top coefficients are listed below:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](http://localhost:8888/notebooks/dsi/dsi-workspace/project-05/images/sample_co_eff_step1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem Statement\n",
    "\n",
    "**By running naive LogisticRegression we received 1.0 on train data set and `0.544` on test data set. Which suggest that the model did poorly on test data and we need to improve on test score. Moreover we need to identify important/salient features that has more importance over our target.**\n",
    "  \n",
    "### Solution Statement\n",
    "\n",
    "**I will build pipeline with LogisticRegression Lasso i.e. l1 with smaller C values (smaller C value means stronger regularization) that will help identify salient features. I am planning run model on Lasso with as many C values to identify highest test score and between 8-10 features.**\n",
    "\n",
    "### Metric\n",
    "\n",
    "**Of the 5-10 models we have identified in the solution statement, we will use the model with higher score and we will use coefficients to identify the salient features, the higher the coefficients the better.** \n",
    "\n",
    "### Benchmark\n",
    "\n",
    "**We need to identify `5-10` salient features.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine\n",
    "import pandas as pd\n",
    "#from os import chdir; chdir('..')\n",
    "from os import chdir; chdir('./lib')\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from project_5 import load_data_from_database, make_data_dict,general_transformer, general_model\n",
    "from sklearn import linear_model\n",
    "#from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression,Lasso\n",
    "from sklearn import metrics\n",
    "from sklearn.feature_selection import SelectKBest, f_regression, f_classif\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/src/dsi/dsi-workspace/project-05/lib\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Load Data, define X and y, Split data in to train and test, and Scale the data for analysis. We will split data in to 50% train and 50% test **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['X_test', 'processes', 'X_train', 'y_train', 'y_test']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "madelon_df = load_data_from_database()\n",
    "del madelon_df[\"index\"]\n",
    "# Define X and y\n",
    "y = madelon_df[\"label\"]\n",
    "X = madelon_df.drop(\"label\",axis=1)\n",
    "data_dict= make_data_dict(X,y,0.5,random_state=42)\n",
    "data_dict = general_transformer(StandardScaler(),data_dict)\n",
    "data_dict.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running LogisticRegression with Lasso and C values\n",
    "### Round 1:  With C 1.0\n",
    "1. Run logistic regression with Lasso and C as 1.0\n",
    "2. Review Score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Score: 0.919 Test Score: 0.533\n"
     ]
    }
   ],
   "source": [
    "# run logistic regression with lasso and C as 1.0\n",
    "model = linear_model.LogisticRegression(penalty = 'l1', C=1.0)\n",
    "data_dict = general_model(model, data_dict)\n",
    "print \"Train Score:\", data_dict[\"train_score\"], \"Test Score:\", data_dict[\"test_score\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The variance between test and train score is significantly higher so we will continue with stronger regularization to get better score.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Round 2 with C as 0.1\n",
    "\n",
    "1. Run logistic regression with Lasso and C as 0.1\n",
    "2. Review Score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Score: 0.798 Test Score: 0.554\n"
     ]
    }
   ],
   "source": [
    "# run logistic regression with lasso and C as 0.1 for EDA purpose\n",
    "model = linear_model.LogisticRegression(penalty = 'l1', C=0.1)\n",
    "data_dict = general_model(model, data_dict)\n",
    "print \"Train Score:\", data_dict[\"train_score\"], \"Test Score:\", data_dict[\"test_score\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The variance between test and train score is still significantly higher so we will continue with stronger regularization to get better score.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Round 3 with C as 0.05\n",
    "\n",
    "1. Run logistic regression with Lasso and C as 0.05\n",
    "2. Review Score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Score: 0.724 Test Score: 0.582\n"
     ]
    }
   ],
   "source": [
    "# run logistic regression with lasso and C as 0.05 for EDA purpose\n",
    "model = linear_model.LogisticRegression(penalty = 'l1', C=0.05)\n",
    "data_dict = general_model(model, data_dict)\n",
    "print \"Train Score:\", data_dict[\"train_score\"], \"Test Score:\", data_dict[\"test_score\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The variance between test and train score is still significantly higher so we will continue with stronger regularization to get better score.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Round 4 with C as 0.03\n",
    "1. Run logistic regression with Lasso and C as 0.03\n",
    "2. Review Score\n",
    "3. Review Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Score: 0.657 Test Score: 0.591\n"
     ]
    }
   ],
   "source": [
    "# run logistic regression with lasso and C as 0.03 for EDA purpose\n",
    "model = linear_model.LogisticRegression(penalty = 'l1', C=0.03)\n",
    "data_dict = general_model(model, data_dict)\n",
    "print \"Train Score:\", data_dict[\"train_score\"], \"Test Score:\", data_dict[\"test_score\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>coef_</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>475</th>\n",
       "      <td>feat_475</td>\n",
       "      <td>0.323946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>feat_048</td>\n",
       "      <td>0.122353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>378</th>\n",
       "      <td>feat_378</td>\n",
       "      <td>0.055020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>307</th>\n",
       "      <td>feat_307</td>\n",
       "      <td>0.053997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>feat_046</td>\n",
       "      <td>0.044833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>424</th>\n",
       "      <td>feat_424</td>\n",
       "      <td>0.032432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>329</th>\n",
       "      <td>feat_329</td>\n",
       "      <td>0.028854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>282</th>\n",
       "      <td>feat_282</td>\n",
       "      <td>0.023329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>feat_116</td>\n",
       "      <td>0.018506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>feat_136</td>\n",
       "      <td>0.007145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>471</th>\n",
       "      <td>feat_471</td>\n",
       "      <td>0.003447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211</th>\n",
       "      <td>feat_211</td>\n",
       "      <td>0.002513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>feat_184</td>\n",
       "      <td>0.001177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>feat_196</td>\n",
       "      <td>0.000914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>335</th>\n",
       "      <td>feat_335</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      feature     coef_\n",
       "475  feat_475  0.323946\n",
       "48   feat_048  0.122353\n",
       "378  feat_378  0.055020\n",
       "307  feat_307  0.053997\n",
       "46   feat_046  0.044833\n",
       "424  feat_424  0.032432\n",
       "329  feat_329  0.028854\n",
       "282  feat_282  0.023329\n",
       "116  feat_116  0.018506\n",
       "136  feat_136  0.007145\n",
       "471  feat_471  0.003447\n",
       "211  feat_211  0.002513\n",
       "184  feat_184  0.001177\n",
       "196  feat_196  0.000914\n",
       "335  feat_335  0.000000"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_coef = []\n",
    "lr_round1 = data_dict[\"processes\"][4]\n",
    "for i,k in enumerate(X.columns):\n",
    "    feature_coef.append([k, lr_round1.coef_[0][i]])\n",
    "df_coef=pd.DataFrame(feature_coef)\n",
    "df_coef.columns = [\"feature\",\"coef_\"]\n",
    "\n",
    "df_coef.sort_values([\"coef_\"],ascending=False).head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**So far we ran below models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "           penalty='l1', random_state=None, solver='liblinear', tol=0.0001,\n",
       "           verbose=0, warm_start=False),\n",
       " LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "           penalty='l1', random_state=None, solver='liblinear', tol=0.0001,\n",
       "           verbose=0, warm_start=False),\n",
       " LogisticRegression(C=0.05, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "           penalty='l1', random_state=None, solver='liblinear', tol=0.0001,\n",
       "           verbose=0, warm_start=False),\n",
       " LogisticRegression(C=0.03, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "           penalty='l1', random_state=None, solver='liblinear', tol=0.0001,\n",
       "           verbose=0, warm_start=False)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dict[\"processes\"][1:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This is a good test score however we have 14 features. We will identify salient features further with higher C values **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Round 5 with C as 0.025 \n",
    "1. Run logistic regression with Lasso and C as 0.025\n",
    "2. Review Score\n",
    "3. Review Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Score: 0.64 Test Score: 0.594\n"
     ]
    }
   ],
   "source": [
    "# run logistic regression with lasso and C as 0.025 for EDA purpose\n",
    "model = linear_model.LogisticRegression(penalty = 'l1', C=0.025)\n",
    "data_dict = general_model(model, data_dict)\n",
    "print \"Train Score:\", data_dict[\"train_score\"], \"Test Score:\", data_dict[\"test_score\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** The test score have improved let't take a look at features **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>coef_</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>475</th>\n",
       "      <td>feat_475</td>\n",
       "      <td>0.295600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>feat_048</td>\n",
       "      <td>0.141942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>307</th>\n",
       "      <td>feat_307</td>\n",
       "      <td>0.023158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>feat_046</td>\n",
       "      <td>0.016922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>378</th>\n",
       "      <td>feat_378</td>\n",
       "      <td>0.007889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>424</th>\n",
       "      <td>feat_424</td>\n",
       "      <td>0.003327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>329</th>\n",
       "      <td>feat_329</td>\n",
       "      <td>0.000256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>334</th>\n",
       "      <td>feat_334</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>331</th>\n",
       "      <td>feat_331</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>332</th>\n",
       "      <td>feat_332</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      feature     coef_\n",
       "475  feat_475  0.295600\n",
       "48   feat_048  0.141942\n",
       "307  feat_307  0.023158\n",
       "46   feat_046  0.016922\n",
       "378  feat_378  0.007889\n",
       "424  feat_424  0.003327\n",
       "329  feat_329  0.000256\n",
       "334  feat_334  0.000000\n",
       "331  feat_331  0.000000\n",
       "332  feat_332  0.000000"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_coef = []\n",
    "lr_round1 = data_dict[\"processes\"][5]\n",
    "for i,k in enumerate(X.columns):\n",
    "    feature_coef.append([k, lr_round1.coef_[0][i]])\n",
    "df_coef=pd.DataFrame(feature_coef)\n",
    "df_coef.columns = [\"feature\",\"coef_\"]\n",
    "\n",
    "df_coef.sort_values([\"coef_\"],ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Our features are reduced to 7. So Let's increase C value by little to see if the score improves and we have atleast 8 features.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Round 6 with C as 0.027\n",
    "1. Run logistic regression with Lasso and C as 0.027\n",
    "2. Review Score\n",
    "3. Review Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Score: 0.649 Test Score: 0.599\n"
     ]
    }
   ],
   "source": [
    "# run logistic regression with lasso and C as 0.027 for EDA purpose\n",
    "model = linear_model.LogisticRegression(penalty = 'l1', C=0.027)\n",
    "data_dict = general_model(model, data_dict)\n",
    "print \"Train Score:\", data_dict[\"train_score\"], \"Test Score:\", data_dict[\"test_score\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The test score is really good @ 60%. Let's review features.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>coef_</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>475</th>\n",
       "      <td>feat_475</td>\n",
       "      <td>0.308325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>feat_048</td>\n",
       "      <td>0.135008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>307</th>\n",
       "      <td>feat_307</td>\n",
       "      <td>0.036766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>feat_046</td>\n",
       "      <td>0.029403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>378</th>\n",
       "      <td>feat_378</td>\n",
       "      <td>0.027127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>424</th>\n",
       "      <td>feat_424</td>\n",
       "      <td>0.016196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>329</th>\n",
       "      <td>feat_329</td>\n",
       "      <td>0.013342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>282</th>\n",
       "      <td>feat_282</td>\n",
       "      <td>0.009831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>feat_116</td>\n",
       "      <td>0.003299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>338</th>\n",
       "      <td>feat_338</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      feature     coef_\n",
       "475  feat_475  0.308325\n",
       "48   feat_048  0.135008\n",
       "307  feat_307  0.036766\n",
       "46   feat_046  0.029403\n",
       "378  feat_378  0.027127\n",
       "424  feat_424  0.016196\n",
       "329  feat_329  0.013342\n",
       "282  feat_282  0.009831\n",
       "116  feat_116  0.003299\n",
       "338  feat_338  0.000000"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_coef = []\n",
    "lr_round1 = data_dict[\"processes\"][6]\n",
    "for i,k in enumerate(X.columns):\n",
    "    feature_coef.append([k, lr_round1.coef_[0][i]])\n",
    "df_coef=pd.DataFrame(feature_coef)\n",
    "df_coef.columns = [\"feature\",\"coef_\"]\n",
    "\n",
    "df_coef.sort_values([\"coef_\"],ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "           penalty='l1', random_state=None, solver='liblinear', tol=0.0001,\n",
       "           verbose=0, warm_start=False),\n",
       " LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "           penalty='l1', random_state=None, solver='liblinear', tol=0.0001,\n",
       "           verbose=0, warm_start=False),\n",
       " LogisticRegression(C=0.05, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "           penalty='l1', random_state=None, solver='liblinear', tol=0.0001,\n",
       "           verbose=0, warm_start=False),\n",
       " LogisticRegression(C=0.03, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "           penalty='l1', random_state=None, solver='liblinear', tol=0.0001,\n",
       "           verbose=0, warm_start=False),\n",
       " LogisticRegression(C=0.025, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "           penalty='l1', random_state=None, solver='liblinear', tol=0.0001,\n",
       "           verbose=0, warm_start=False),\n",
       " LogisticRegression(C=0.027, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "           penalty='l1', random_state=None, solver='liblinear', tol=0.0001,\n",
       "           verbose=0, warm_start=False)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dict[\"processes\"][1:7]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We ran 6 models with LogisticRegression. With penalty Lasso and C value as 0.027 we found the highest test score with 8 features. We will use this model and compare that against other models e.g. SelectKBest, KNN and GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation\n",
    "\n",
    "Implement the following code pipeline using the functions you write in `lib/project_5.py`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"assets/identify_features.png\" width=\"600px\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
